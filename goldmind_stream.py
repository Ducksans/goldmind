from langchain_community.llms import Ollama
from langchain_core.callbacks.base import BaseCallbackHandler

class StreamingStdoutHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(token, end="", flush=True)

def chat_with_goldmind(prompt):
    llm = Ollama(
        model="llama3",
        streaming=True,
        callbacks=[StreamingStdoutHandler()]
    )
    print("ğŸ¤– ê¸ˆê°•ì´ì˜ ì‘ë‹µ:")
    llm.invoke(prompt)

if __name__ == "__main__":
    user_input = input("ğŸ¤– ê¸ˆê°•ì´ì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”: ")
    chat_with_goldmind(user_input)
